{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7ba0da-a427-495d-b8eb-36ccb5606797",
   "metadata": {},
   "source": [
    "# Papers without Code EDA and Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b14a8fd-69f2-4df4-acf1-bed5e89ee157",
   "metadata": {},
   "source": [
    "Very minimal exploratory data analysis on the dataset constructed for testing `papers-without-code`. The purpose is to quantify \"what proportion of papers likely use code for their paper but do not link to it in anyway?\" Then, analysis on how the package performs.\n",
    "\n",
    "You can read about the dataset construction on the [data README](./README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93753407-5b74-4059-b701-6996ab924a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some basic data reading and prep\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "df = pd.read_csv(\"annotated.csv\", dtype={\"id\": str})\n",
    "\n",
    "# We have only annotated 25 so far\n",
    "df = df[:25]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc174d94-3c2e-4090-97c8-387787849a9f",
   "metadata": {},
   "source": [
    "## \"How many papers likely use code?\"\n",
    "\n",
    "Out of the 25 papers annotated so far, 20 \"likely used code\" as a part of their work in completing the paper. These other papers are commonly math or theory papers where they introduce an algorithm via mathematical notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4803d-789d-4c26-9ce1-478f7f8a8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df.best_guess_paper_used_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e104e4-d1ff-489d-b3b3-e49ed3da6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the repos where we don't think code was used\n",
    "# Usually math or theory papers\n",
    "df = df[df.best_guess_paper_used_code == \"yes\"]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc47747-7183-4ba0-9fd2-f03127956f20",
   "metadata": {},
   "source": [
    "## \"Of the 20, how many papers can we find repositories for?\"\n",
    "\n",
    "Out of the remaining 20 papers, we can find 13 related repositories. In the cases where we can't find repositories they either were not discoverable within the ~10 minutes I gave to searching for each repository or in one case after searching I assume the code is private because both authors are from private industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760b066-ac01-4c0a-b8bc-02374ac2d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"code_found\"] = ~df.code_repository_link.isna()\n",
    "sns.countplot(x=df.code_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0a15a-dfd0-48bb-97ab-e5bb265470a6",
   "metadata": {},
   "source": [
    "## \"How do the papers break down by if code was found AND the code had to be manually found (it _wasn't_ linked in the paper)?\"\n",
    "\n",
    "Of the papers where related code was found, 8 of the papers provided links directly to the code and 5 of the papers I had to manually search for repositories for.\n",
    "\n",
    "Note on the odd case of \"no code was found but a code repository _was_ linked in the paper\" is that the code has since been deleted (or was never published) -- however, I found a similar repository authored by one of the authors that I feel would be useful to serve back to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ad22f-cdb3-49d9-b183-436c10768d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"code_found\", hue=\"code_repository_linked_in_paper\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579e1c2-78e3-450a-8388-c0ee167521f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.code_found == False) & (df.code_repository_linked_in_paper == \"yes\")].iloc[0].comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c77dbf-b798-4add-a8f1-a13b125a64d1",
   "metadata": {},
   "source": [
    "## \"How many repositories can we find with our automated methods?\"\n",
    "\n",
    "Using the automated search, we only found four repositories that exactly match the ones the paper used for their method or analysis. Of those four, they all came from papers which provided links to their repository in their own paper, but, we didn't even find the fifth paper that did this. We also didn't find any of the repositories which we manually found repositories for.\n",
    "\n",
    "_The web application uses the same code as the Python package so we can do this analysis in this notebook._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a2dd2-93a3-4566-a992-56523c386b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe with just the papers where we manually found code\n",
    "code_found = df.loc[df.code_found]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a775e-d6ff-4a6d-84e2-ebe069d3ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from papers_without_code import search_for_repos\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Attempt to find repos for each paper and sort their results\n",
    "matching_results_benchmark_rows = []\n",
    "for _, row in tqdm(code_found.iterrows()):\n",
    "    # Prepend the search with arxiv\n",
    "    paper_repo_results = search_for_repos(f\"arxiv:{row.id}\")\n",
    "\n",
    "    # Check all results\n",
    "    found_match = False\n",
    "    match_category = \"\"\n",
    "    for i, repo in enumerate(paper_repo_results):\n",
    "        # Check for match\n",
    "        if repo.link == row.code_repository_link:\n",
    "            found_match = True\n",
    "            if i == 0:\n",
    "                match_category = \"first\"\n",
    "            elif i < 3:\n",
    "                match_category = \"top three\"\n",
    "            elif i < 5:\n",
    "                match_category = \"top five\"\n",
    "            else:\n",
    "                match_category = \"after top five\"\n",
    "\n",
    "            # Break out and finish up this paper\n",
    "            break\n",
    "\n",
    "    # Update row with info\n",
    "    if found_match:\n",
    "        row[\"match\"] = match_category\n",
    "    else:\n",
    "        row[\"match\"] = \"not found\"\n",
    "\n",
    "    # Add row to new dataframe\n",
    "    matching_results_benchmark_rows.append(row)\n",
    "    \n",
    "matching_results_benchmark = pd.DataFrame(matching_results_benchmark_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae7d82-a2f2-4158-9c49-9bc5d0fa0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_results_benchmark[\n",
    "    matching_results_benchmark.match != \"not found\"\n",
    "][[\"id\", \"code_repository_linked_in_paper\", \"match\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b566df-836f-4b7c-ba06-e1ab3ee6f7cc",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "### The Bad\n",
    "\n",
    "Our search isn't that great at finding the _exact_ matching repository to the paper. I think a large portion of the failure comes because of two things:\n",
    "\n",
    "1. GitHub's API has an incredibly strict rate limit. In order to make the search work efficiently, I am using repository searches and not \"code searches\". By looking at the tqdm average from the prior processing cell, we can see that on average searches were completed in ~20 seconds. This is the tradeoff I had to make.\n",
    "\n",
    "2. If you look at my original annotation data, I left notes as to how I found multiple of the repositories. In many cases I was searching for one of the paper authors GitHub profile and then looking at only their repositories. In this current search, we are just doing keyword searches across all repositories.\n",
    "\n",
    "### The Good\n",
    "\n",
    "It isn't all for waste in my opinion though. While the original intent was to find GitHub repositories which are the exact match for a paper, we are able to find repositories that are similar or may still be useful to the user. I don't have any qualitative data to back this up but from my own experiences, I have found repositories related to my own research that I have never seen before but are incredibly relevent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
